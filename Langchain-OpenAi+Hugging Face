pip install langchain openai huggingface_hub
# openai_huggingface_langchain.py

from langchain.llms import OpenAI, HuggingFaceHub
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
import os

# Set your keys
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "your-huggingface-token"

# 1Ô∏è‚É£ OpenAI for answering a question
openai_llm = OpenAI(model_name="gpt-3.5-turbo", temperature=0.7)

qa_prompt = PromptTemplate(
    input_variables=["question"],
    template="You are a helpful assistant. Answer the following question:\n{question}"
)

qa_chain = LLMChain(llm=openai_llm, prompt=qa_prompt)

# 2Ô∏è‚É£ HuggingFace model for summarizing the OpenAI response
hf_llm = HuggingFaceHub(repo_id="google/flan-t5-base", model_kwargs={"temperature": 0.5, "max_length": 100})

summary_prompt = PromptTemplate(
    input_variables=["answer"],
    template="Summarize this response in simple terms:\n\n{answer}"
)

summary_chain = LLMChain(llm=hf_llm, prompt=summary_prompt)

# üîÅ Combined pipeline
def hybrid_response_pipeline(question):
    answer = qa_chain.run(question=question)
    summary = summary_chain.run(answer=answer)
    return answer, summary

# ‚ñ∂Ô∏è Example
if __name__ == "__main__":
    question = input("Ask me anything: ")
    full_answer, short_summary = hybrid_response_pipeline(question)
    
    print("\nüîπ OpenAI Full Answer:\n", full_answer)
    print("\nüî∏ HuggingFace Summary:\n", short_summary)
